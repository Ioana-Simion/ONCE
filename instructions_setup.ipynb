{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Reproduction Step-by-Step Setup Guide\n",
    "\n",
    "This notebook provides a comprehensive step-by-step guide to set up and reproduce our experiments. By following these instructions, you will be able to download the necessary models, convert their weights, and prepare the data required for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up your environment, first run the following commands:\n",
    "```bash\n",
    "python -m venv venv\n",
    "Set-ExecutionPolicy Unrestricted -Scope Process\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "Next, navigate to `ebnerd-benchmark` and run:\n",
    "```bash\n",
    "pip install .\n",
    "```\n",
    "\n",
    "After that, navigate back to the root directory and first upgrade torch to a cuda version, following the instructions [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "Lastly, to add the rest of the required packages with the correct versions, run:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download the Llama-2 model\n",
    "\n",
    "First, download Llama-2 model from the Meta website: https://llama.meta.com/llama-downloads/. Filling in the form here will give you a link you can use to run this script https://github.com/meta-llama/llama/blob/main/download.sh (make sure you install wget - https://gnuwin32.sourceforge.net/packages/wget.htm - if you don't have it already before running this script).\n",
    "\n",
    "\n",
    "After downloading, make sure the model is saved in a folder with the following structure:\n",
    "\n",
    "llama\n",
    "- **7B**\n",
    "  - `checklist.chk`\n",
    "  - `config.json`\n",
    "  - `consolidated.00.pth`\n",
    "  - `params.json`\n",
    "- `LICENSE`\n",
    "- `tokenizer_checklist.chk`\n",
    "- `tokenizer.model`\n",
    "- `USE_POLICY.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert Llama weights to HuggingFace interface\n",
    "\n",
    "Next, convert the Llama weights to the HuggingFace interface using the `convert_llama_weights_to_hf` script with the following arguments:\n",
    "```bash\n",
    "python convert_llama_weights_to_hf.py --input_dir llama --model_size 7B --output_dir llama_converted --llama_version 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Create the weights embeddings\n",
    "\n",
    "Create the weights embeddings using the following script:\n",
    "\n",
    "```bash\n",
    "python convert_llama_converted_to_token_npy.py\n",
    "```\n",
    "\n",
    "This will create a llama-token.npy file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data\n",
    "\n",
    "Depending on the embedding layer implementation you want to reproduce, you can choose one of the following methods to tokenize your data. Each method provides a unique way of generating embeddings for your data.\n",
    "<!-- \n",
    "1. Tokenize with BERT\n",
    "2. Tokenize with Llama\n",
    "3. Tokenize both with BERT and Llama and combine the embeddings for the news -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Method 1: Tokenize with BERT\n",
    "\n",
    "To leverage BERT as the LLM implementation of the embedding layer, you can use `process\\eb-nerd\\processor.py`. Before running this script, make sure to change these paths in the main function:\n",
    "```python\n",
    "processor = Processor(\n",
    "    data_dir=\"ebnerd-benchmark/data/ebnerd_small\", # PATH to your data\n",
    "    store_dir=\"ebnerd-benchmark/data/tokenized_bert\" # PATH to the directory where you want to save the tokenized data\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Method 2: Tokenize with Llama\n",
    "\n",
    "To leverage Llama as the LLM implementation of the embedding layer, you can use `process\\eb-nerd\\processor_llama.py`. Before running this script, make sure to change these paths in the main function:\n",
    "```python\n",
    "processor = Processor(\n",
    "    data_dir=\"ebnerd-benchmark/data/ebnerd_small\", # PATH to your data\n",
    "    store_dir=\"ebnerd-benchmark/data/tokenized_llama\" # PATH to the directory where you want to save the tokenized data\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Method 3: Tokenize with both BERT and Llama\n",
    "\n",
    "In order to leverage both LLMs for the (news) items representations, you can use `process\\eb-nerd\\fusion.py`. Before running this script, make sure to change these paths:\n",
    "```python\n",
    "news = UniDep(os.path.join(\"ebnerd-benchmark/data/tokenized_bert\", 'news')) # PATH to the directory where data tokenized with BERT is saved\n",
    "news_llama = UniDep('ebnerd-benchmark/data/tokenized_llama/news-llama') # PATH to the directory where data tokenized with Llama is saved\n",
    "...\n",
    "news.export('ebnerd-benchmark/data/news-fusion') # PATH to the directory where you want to save the combined representation of the (news) items\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To integrate GENRE-generated data, similar operations should be conducted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
