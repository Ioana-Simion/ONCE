name: ebnerd-LLAMA-NAML.D${model.config.hidden_size}.L${model.config.item_config.layer_split}.Lora${model.config.item_config.lora}
meta:
  item: Llama
  user: Ada
  predictor: Dot
config:
  use_neg_sampling: false
  use_item_content: true
  max_item_content_batch_size: ${max_item_batch_size}$
  same_dim_transform: false
  embed_hidden_size: ${embed_hidden_size}$
  hidden_size: ${hidden_size}$
  item_config:
    llm_dir: llama_converted
    layer_split: ${layer}$
    lora: ${lora}$
    weights_dir: data/EB-NeRD/llama-7b-split
  user_config:
    num_attention_heads: 12
    inputer_config:
      use_cls_token: false
      use_sep_token: false
