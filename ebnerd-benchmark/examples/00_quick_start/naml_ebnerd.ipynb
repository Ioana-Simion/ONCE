{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader, NAMLDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel\n",
    "\n",
    "# TODO make a notebook with it\n",
    "from ebrec.models.newsrec.model_config import hparams_naml\n",
    "from ebrec.models.newsrec.naml import NAMLModel\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/data\")\n",
    "DATASPLIT = \"ebnerd_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "\n",
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(2)\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from ebrec.models.newsrec.dataloader import (\n",
    "    LSTURDataLoader,\n",
    "    NAMLDataLoader,\n",
    "    NRMSDataLoader,\n",
    ")\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    create_user_id_to_int_mapping,\n",
    ")\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_CATEGORY_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "from ebrec.utils._python import create_lookup_dict, time_it\n",
    "\n",
    "TOKEN_COL = \"tokens\"\n",
    "N_SAMPLES = \"n\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# LOAD DATA:\n",
    "PATH_DATA = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/test/data\")\n",
    "#PATH_DATA = Path(\"test/data\")\n",
    "df_articles = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"ebnerd\", \"articles.parquet\"))\n",
    "    .select(pl.col(DEFAULT_ARTICLE_ID_COL, DEFAULT_CATEGORY_COL))\n",
    "    .with_columns(pl.Series(TOKEN_COL, np.random.randint(0, 20, (1, 10))))\n",
    "    .collect()\n",
    ")\n",
    "df_history = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"ebnerd\", \"history.parquet\"))\n",
    "    .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "    .with_columns(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.tail(3))\n",
    ")\n",
    "df_behaviors = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"ebnerd\", \"behaviors.parquet\"))\n",
    "    .select(DEFAULT_USER_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "    .with_columns(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.len().alias(N_SAMPLES))\n",
    "    .join(df_history, on=DEFAULT_USER_COL, how=\"left\")\n",
    "    .collect()\n",
    "    .pipe(create_binary_labels_column)\n",
    ")\n",
    "# => MAPPINGS:\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=TOKEN_COL\n",
    ")\n",
    "user_mapping = create_user_id_to_int_mapping(df=df_behaviors)\n",
    "# => NPRATIO IMPRESSION - SAME LENGTHS:\n",
    "df_behaviors_train = df_behaviors.filter(pl.col(N_SAMPLES) == pl.col(N_SAMPLES).min())\n",
    "# => FOR TEST-DATALOADER\n",
    "label_lengths = df_behaviors[DEFAULT_INVIEW_ARTICLES_COL].list.len().to_list()\n",
    "\n",
    "body_mapping = article_mapping\n",
    "category_mapping = create_lookup_dict(\n",
    "    df_articles.select(pl.col(DEFAULT_CATEGORY_COL).unique()).with_row_index(\n",
    "        \"row_nr\"\n",
    "    ),\n",
    "    key=DEFAULT_CATEGORY_COL,\n",
    "    value=\"row_nr\",\n",
    ")\n",
    "subcategory_mapping = category_mapping\n",
    "\n",
    "train_dataloader = NAMLDataLoader(\n",
    "    behaviors=df_behaviors_train,\n",
    "    article_dict=article_mapping,\n",
    "    body_mapping=body_mapping,\n",
    "    category_mapping=category_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    subcategory_mapping=subcategory_mapping,\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "batch = train_dataloader.__iter__().__next__()\n",
    "\n",
    "assert train_dataloader.__len__() == int(np.ceil(df_behaviors_train.shape[0] / 100))\n",
    "assert len(batch) == 2, \"There should be two outputs: (inputs, labels)\"\n",
    "assert (\n",
    "    len(batch[0]) == 8\n",
    "), \"NAML has two outputs (his_input_title,his_input_body,his_input_vert,his_input_subvert,pred_input_title,pred_input_body,pred_input_vert,pred_input_subvert)\"\n",
    "\n",
    "for type_in_batch in batch[0][0]:\n",
    "    assert isinstance(\n",
    "        type_in_batch.ravel()[0], np.integer\n",
    "    ), \"Expected output to be integer; used for lookup value\"\n",
    "\n",
    "assert isinstance(\n",
    "    batch[1].ravel()[0], np.integer\n",
    "), \"Expected output to be integer; this is label\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TITLE_SIZE = 30\n",
    "DEFAULT_BODY_SIZE = 40\n",
    "UNKNOWN_TITLE_VALUE = [0] * DEFAULT_TITLE_SIZE\n",
    "UNKNOWN_BODY_VALUE = [0] * DEFAULT_BODY_SIZE\n",
    "\n",
    "DEFAULT_DOCUMENT_SIZE = 768\n",
    "\n",
    "\n",
    "class hparams_naml:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 50\n",
    "    body_size: int = DEFAULT_BODY_SIZE\n",
    "    vert_num: int = 100\n",
    "    vert_emb_dim: int = 10\n",
    "    subvert_num: int = 100\n",
    "    subvert_emb_dim: int = 10\n",
    "    # MODEL ARCHITECTURE\n",
    "    dense_activation: str = \"relu\"\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 10)\n",
      "(100, 3, 10)\n",
      "(100, 3, 1)\n",
      "(100, 3, 1)\n",
      "(100, 4, 10)\n",
      "(100, 4, 10)\n",
      "(100, 4, 1)\n",
      "(100, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "x_sample, y_sample = next(iter(train_dataloader))\n",
    "for item in x_sample:\n",
    "    print(item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(x, desired_length, feature_size):\n",
    "    # Pad with zeros or truncate to desired_length, adjusting feature size as needed\n",
    "    padded = np.zeros((len(x), desired_length, feature_size))\n",
    "    for i, sample in enumerate(x):\n",
    "        padded[i, :min(len(sample), desired_length), :min(sample.shape[1], feature_size)] = sample[:desired_length, :feature_size]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_221 (InputLayer)      [(None, None, 30)]           0         []                            \n",
      "                                                                                                  \n",
      " input_222 (InputLayer)      [(None, None, 40)]           0         []                            \n",
      "                                                                                                  \n",
      " input_223 (InputLayer)      [(None, None, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " input_224 (InputLayer)      [(None, None, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " input_217 (InputLayer)      [(None, 50, 30)]             0         []                            \n",
      "                                                                                                  \n",
      " input_218 (InputLayer)      [(None, 50, 40)]             0         []                            \n",
      "                                                                                                  \n",
      " input_219 (InputLayer)      [(None, 50, 1)]              0         []                            \n",
      "                                                                                                  \n",
      " input_220 (InputLayer)      [(None, 50, 1)]              0         []                            \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenat  (None, None, 72)             0         ['input_221[0][0]',           \n",
      " e)                                                                  'input_222[0][0]',           \n",
      "                                                                     'input_223[0][0]',           \n",
      "                                                                     'input_224[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenat  (None, 50, 72)               0         ['input_217[0][0]',           \n",
      " e)                                                                  'input_218[0][0]',           \n",
      "                                                                     'input_219[0][0]',           \n",
      "                                                                     'input_220[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed_25 (TimeD  (None, None, 400)            1983983   ['concatenate_49[0][0]']      \n",
      " istributed)                                              36                                      \n",
      "                                                                                                  \n",
      " user_encoder (Functional)   (None, 400)                  1984787   ['concatenate_48[0][0]']      \n",
      "                                                          36                                      \n",
      "                                                                                                  \n",
      " dot_24 (Dot)                (None, None)                 0         ['time_distributed_25[0][0]', \n",
      "                                                                     'user_encoder[0][0]']        \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, None)                 0         ['dot_24[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198478736 (757.14 MB)\n",
      "Trainable params: 198478736 (757.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Samples shape:  (100, 3, 10) (4,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_24\" is incompatible with the layer: expected shape=(None, 50, 30), found shape=(None, 3, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x_sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, y_sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#x_padded = pad_sequences(x_sample, 50, 30)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(x_sample, y_sample)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6vpl2423.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_24\" is incompatible with the layer: expected shape=(None, 50, 30), found shape=(None, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "config = hparams_naml\n",
    "\n",
    "# Model\n",
    "model = NAMLModel(hparams=config, word2vec_embedding=word2vec_embedding)\n",
    "model.model.summary()\n",
    "x_sample, y_sample = next(iter(train_dataloader))\n",
    "#print(\"Samples shape: \", x_sample[0].shape, y_sample[0].shape)\n",
    "#x_padded = pad_sequences(x_sample, 50, 30)\n",
    "model.model.fit(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"NAML\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    ")\n",
    "_ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_validation = model.scorer.predict(val_dataloader)\n",
    "pred_validation = model.model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_validation.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_validation[\"ranked_scores\"],\n",
    "    path=\"downloads/predictions.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
