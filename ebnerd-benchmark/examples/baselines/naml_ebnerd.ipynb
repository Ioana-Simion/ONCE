{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms, hparams_naml\n",
    "from ebrec.models.newsrec.naml import NAMLModel\n",
    "from ebrec.utils._articles import (\n",
    "    create_article_id_to_value_mapping,\n",
    "    convert_text2encoding_with_transformers\n",
    ")\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    create_user_id_to_int_mapping\n",
    ")\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_CATEGORY_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._python import create_lookup_dict, time_it, write_submission_file, rank_predictions_by_score, create_lookup_objects\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsrecDataLoader(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A DataLoader for news recommendation.\n",
    "    \"\"\"\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history_column: str\n",
    "    article_dict: dict[int, any]\n",
    "    unknown_representation: str\n",
    "    eval_mode: bool = False\n",
    "    batch_size: int = 32\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL\n",
    "    labels_col: str = DEFAULT_LABELS_COL\n",
    "    user_col: str = DEFAULT_USER_COL\n",
    "    kwargs: field(default_factory=dict) = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method. Loads the data and sets additional attributes.\n",
    "        \"\"\"\n",
    "        self.lookup_article_index, self.lookup_article_matrix = create_lookup_objects(\n",
    "            self.article_dict, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        self.unknown_index = [0]\n",
    "        self.X, self.y = self.load_data()\n",
    "        if self.kwargs is not None:\n",
    "            self.set_kwargs(self.kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise ValueError(\"Function '__getitem__' needs to be implemented.\")\n",
    "\n",
    "    def load_data(self) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        X = self.behaviors.drop(self.labels_col).with_columns(\n",
    "            pl.col(self.inview_col).list.len().alias(\"n_samples\")\n",
    "        )\n",
    "        y = self.behaviors[self.labels_col]\n",
    "        return X, y\n",
    "\n",
    "    def set_kwargs(self, kwargs: dict):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class NAMLDataLoader(NewsrecDataLoader):\n",
    "    \"\"\"\n",
    "    Eval mode not implemented\n",
    "    \"\"\"\n",
    "\n",
    "    unknown_category_value: int = 0\n",
    "    unknown_subcategory_value: int = 0\n",
    "    body_mapping: dict[int, list[int]] = None\n",
    "    category_mapping: dict[int, int] = None\n",
    "    subcategory_mapping: dict[int, int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.title_prefix = \"title_\"\n",
    "        self.body_prefix = \"body_\"\n",
    "        self.category_prefix = \"category_\"\n",
    "        self.subcategory_prefix = \"subcategory_\"\n",
    "        (\n",
    "            self.lookup_article_index_body,\n",
    "            self.lookup_article_matrix_body,\n",
    "        ) = create_lookup_objects(\n",
    "            self.body_mapping, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        # if self.eval_mode:\n",
    "        #     raise ValueError(\"'eval_mode = True' is not implemented for NAML\")\n",
    "\n",
    "        return super().__post_init__()\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> tuple[pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Special case for NAML as it requires body-encoding, verticals, & subvertivals\n",
    "        \"\"\"\n",
    "        # =>\n",
    "        title = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        body = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        category = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        subcategory = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        return (\n",
    "            pl.DataFrame()\n",
    "            .with_columns(title.select(pl.all().name.prefix(self.title_prefix)))\n",
    "            .with_columns(body.select(pl.all().name.prefix(self.body_prefix)))\n",
    "            .with_columns(category.select(pl.all().name.prefix(self.category_prefix)))\n",
    "            .with_columns(\n",
    "                subcategory.select(pl.all().name.prefix(self.subcategory_prefix))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        batch_y = np.array(batch_y.to_list())\n",
    "        his_input_title = np.array(batch_X[self.title_prefix + self.history_column].to_list())\n",
    "        his_input_body = np.array(batch_X[self.body_prefix + self.history_column].to_list())\n",
    "\n",
    "        # =>\n",
    "        pred_input_title = np.array(batch_X[self.title_prefix + self.inview_col].to_list())   \n",
    "        pred_input_body = np.array(batch_X[self.body_prefix + self.inview_col].to_list())\n",
    "\n",
    "        # =>\n",
    "        his_input_title = np.squeeze(self.lookup_article_matrix[his_input_title], axis=2)\n",
    "        pred_input_title = np.squeeze(self.lookup_article_matrix[pred_input_title], axis=2)\n",
    "        his_input_body = np.squeeze(self.lookup_article_matrix_body[his_input_body], axis=2)\n",
    "        pred_input_body = np.squeeze(self.lookup_article_matrix_body[pred_input_body], axis=2)\n",
    "        # =>\n",
    "\n",
    "        his_input_vert = np.array(batch_X[self.category_prefix + self.history_column].to_list())[:, :, np.newaxis]\n",
    "        his_input_subvert = np.array(batch_X[self.subcategory_prefix + self.history_column].to_list())[:, :, np.newaxis]\n",
    "        pred_input_vert = np.array(batch_X[self.category_prefix + self.inview_col].to_list())[:, :, np.newaxis]\n",
    "        pred_input_subvert = np.array(batch_X[self.subcategory_prefix + self.inview_col].to_list())[:, :, np.newaxis]\n",
    "\n",
    "        if self.eval_mode: # Added the eval_mode condition\n",
    "            return (\n",
    "                his_input_title,\n",
    "                his_input_body,\n",
    "                his_input_vert,\n",
    "                his_input_subvert,\n",
    "                pred_input_title,\n",
    "                pred_input_body,\n",
    "                pred_input_vert,\n",
    "                pred_input_subvert,\n",
    "            ), \n",
    "        else:\n",
    "            return (\n",
    "                his_input_title,\n",
    "                his_input_body,\n",
    "                his_input_vert,\n",
    "                his_input_subvert,\n",
    "                pred_input_title,\n",
    "                pred_input_body,\n",
    "                pred_input_vert,\n",
    "                pred_input_subvert,\n",
    "            ), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(PATH_DATA, do_eval):\n",
    "        \n",
    "    TOKEN_COL = \"tokens\"\n",
    "    N_SAMPLES = \"n\"\n",
    "    BATCH_SIZE = 100\n",
    "    df_articles = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"../articles.parquet\"))\n",
    "        .select(pl.col(DEFAULT_ARTICLE_ID_COL, DEFAULT_CATEGORY_COL))\n",
    "        .with_columns(pl.Series(TOKEN_COL, np.random.randint(0, 20, (1, 10))))\n",
    "        .collect()\n",
    "    )\n",
    "    df_history = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .with_columns(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.tail(3))\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"behaviors.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "        .with_columns(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.len().alias(N_SAMPLES))\n",
    "        .join(df_history, on=DEFAULT_USER_COL, how=\"left\")\n",
    "        .collect()\n",
    "        .pipe(create_binary_labels_column)\n",
    "    )\n",
    "    # => MAPPINGS:\n",
    "    article_mapping = create_article_id_to_value_mapping(\n",
    "        df=df_articles, value_col=TOKEN_COL\n",
    "    )\n",
    "    user_mapping = create_user_id_to_int_mapping(df=df_behaviors)\n",
    "    # => NPRATIO IMPRESSION - SAME LENGTHS:\n",
    "    df_behaviors_train = df_behaviors.filter(pl.col(N_SAMPLES) == pl.col(N_SAMPLES).min())\n",
    "    # => FOR TEST-DATALOADER\n",
    "    label_lengths = df_behaviors[DEFAULT_INVIEW_ARTICLES_COL].list.len().to_list()\n",
    "\n",
    "    body_mapping = article_mapping\n",
    "    category_mapping = create_lookup_dict(\n",
    "        df_articles.select(pl.col(DEFAULT_CATEGORY_COL).unique()).with_row_index(\n",
    "            \"row_nr\"\n",
    "        ),\n",
    "        key=DEFAULT_CATEGORY_COL,\n",
    "        value=\"row_nr\",\n",
    "    )\n",
    "    subcategory_mapping = category_mapping\n",
    "\n",
    "    dataloader = NAMLDataLoader(\n",
    "        behaviors=df_behaviors_train,\n",
    "        eval_mode=do_eval,\n",
    "        article_dict=article_mapping,\n",
    "        body_mapping=body_mapping,\n",
    "        category_mapping=category_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        subcategory_mapping=subcategory_mapping,\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    return dataloader, df_behaviors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA:\n",
    "PATH_DATA = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/data/ebnerd_demo/train\")\n",
    "do_eval = False\n",
    "train_dataloader, df_behaviors_train = make_data_loader(PATH_DATA, do_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA:\n",
    "PATH_DATA = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/data/ebnerd_demo/validation\")\n",
    "do_eval = True\n",
    "val_dataloader, df_behaviors_val = make_data_loader(PATH_DATA, do_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TITLE_SIZE = 10 #30\n",
    "DEFAULT_BODY_SIZE = 10 #40\n",
    "UNKNOWN_TITLE_VALUE = [0] * DEFAULT_TITLE_SIZE\n",
    "UNKNOWN_BODY_VALUE = [0] * DEFAULT_BODY_SIZE\n",
    "\n",
    "DEFAULT_DOCUMENT_SIZE = 768\n",
    "\n",
    "\n",
    "class hparams_naml:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 3 #50\n",
    "    body_size: int = DEFAULT_BODY_SIZE\n",
    "    vert_num: int = 100\n",
    "    vert_emb_dim: int = 10\n",
    "    subvert_num: int = 100\n",
    "    subvert_emb_dim: int = 10\n",
    "    # MODEL ARCHITECTURE\n",
    "    dense_activation: str = \"relu\"\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.0001\n",
    "\n",
    "\n",
    "config = hparams_naml()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 303s 734ms/step - loss: 1.6198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2028168f210>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "model = NAMLModel(hparams=config, word2vec_embedding=word2vec_embedding)\n",
    "# model.model.summary()\n",
    "model.model.fit(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402/402 [==============================] - 42s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.49998132191064404,\n",
       "    \"mrr\": 0.4534452773488735,\n",
       "    \"ndcg@5\": 0.5874333967041713,\n",
       "    \"ndcg@10\": 0.5874333967041713\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_behaviors_val, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_behaviors_val[DEFAULT_USER_COL]\n",
    ")\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
